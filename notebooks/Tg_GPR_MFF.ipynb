{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a57cce1-16e9-4ede-9d86-75a06cfda637",
   "metadata": {},
   "source": [
    "# Gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3d35d-3832-4d63-bbc4-828ce0c761c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from math import sqrt\n",
    "\n",
    "#from keras_tuner.tuners import RandomSearch\n",
    "# from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from collections import Counter \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr\n",
    "from numpy.random import seed\n",
    "import scipy\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc731f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Tg.csv\")\n",
    "\n",
    "molecules = df.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "HashCode = []\n",
    "for i in fp_n:\n",
    "    for j in i.keys():\n",
    "        HashCode.append(j)\n",
    "                \n",
    "unique_set = set(HashCode)\n",
    "unique_list = list(unique_set)\n",
    "Corr_df = pd.DataFrame(unique_list).reset_index()\n",
    "                \n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "        my_finger[index] = polymer[key]\n",
    "    MY_finger.append(my_finger)\n",
    "X = pd.DataFrame(MY_finger)\n",
    "\n",
    "# filter input into the most popular X substructures\n",
    "Zero_Sum = (X == 0).astype(int).sum()\n",
    "NumberOfZero = 6862\n",
    "print(len(Zero_Sum[Zero_Sum < NumberOfZero]))\n",
    "\n",
    "Columns = Zero_Sum[Zero_Sum < NumberOfZero].index\n",
    "Substructure_list = list(polymer.keys())\n",
    "X_count = X[Columns]\n",
    "\n",
    "Y = df['Tg'].values\n",
    "\n",
    "xtrain, xtemp, ytrain, ytemp = train_test_split(X_count, Y, test_size=0.2, random_state=11)\n",
    "xval, xtest, yval, ytest = train_test_split(xtemp, ytemp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of materials (rows) and the length of the first fingerprint (columns)\n",
    "print(f\"Number of materials: {len(MY_finger)}\")\n",
    "print(f\"Length of the first fingerprint: {len(MY_finger[0]) if MY_finger else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911483d-de46-4312-a28f-9a3bb272000e",
   "metadata": {},
   "source": [
    "## GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to NumPy arrays for xtrain and xtest if they are not already\n",
    "xtrain_np = xtrain if isinstance(xtrain, np.ndarray) else xtrain.values\n",
    "xtest_np = xtest if isinstance(xtest, np.ndarray) else xtest.values\n",
    "\n",
    "# Reshape ytrain and ytest if they are not already 2D arrays\n",
    "ytrain_np = ytrain.reshape(-1, 1) if len(ytrain.shape) == 1 else ytrain\n",
    "ytest_np = ytest.reshape(-1, 1) if len(ytest.shape) == 1 else ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13caec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernel\n",
    "# kernel = GPy.kern.RBF(input_dim=xtrain_np.shape[1], variance=1., lengthscale=1.)\n",
    "kernel = GPy.kern.Matern32(input_dim=xtrain_np.shape[1], variance=1., lengthscale=1.)\n",
    "kernel += GPy.kern.White(xtrain_np.shape[1], variance=1.)\n",
    "\n",
    "# Create GPR model with normalized data\n",
    "model_GPR = GPy.models.GPRegression(xtrain_np, ytrain_np, kernel)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "model_GPR.optimize(messages=True, max_iters=500)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb60e7-9793-41cf-95fa-df345a708acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with normalized features\n",
    "mean_train, train_variance = model_GPR.predict(xtrain_np)\n",
    "mean_test, test_variance = model_GPR.predict(xtest_np)\n",
    "\n",
    "std_train = np.sqrt(train_variance)\n",
    "std_test = np.sqrt(test_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d596f-2e78-4a09-8022-169e53f46c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = mean_train[:, 0]\n",
    "std_train = std_train[:, 0]\n",
    "mean_test = mean_test[:, 0]\n",
    "std_test = std_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da29e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the training set\n",
    "abs_error_train = abs(ytrain - mean_train)\n",
    "spearman_corr_train, p_value_train = spearmanr(abs_error_train, std_train)\n",
    "\n",
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the test set\n",
    "abs_error_test = abs(ytest - mean_test)\n",
    "spearman_corr_test, p_value_test = spearmanr(abs_error_test, std_test)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results = {\n",
    "    'Spearman_Correlation': [spearman_corr_train, spearman_corr_test],\n",
    "    'p_value': [p_value_train, p_value_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame with 'Train' and 'Test' as index\n",
    "spearman_df = pd.DataFrame(spearman_results, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(abs_error_train, std_train, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (train)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (train)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (train Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(abs_error_test, std_test, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (Test)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (Test)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (test Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33093f76-d7f1-46b1-b3ac-dc935913f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "ytrain_1d = np.ravel(ytrain)\n",
    "ytest_1d = np.ravel(ytest)\n",
    "mean_train_1d = np.ravel(mean_train)\n",
    "std_train_1d = np.ravel(std_train)\n",
    "mean_test_1d = np.ravel(mean_test)\n",
    "std_test_1d = np.ravel(std_test)\n",
    "\n",
    "# Metric calculation\n",
    "mae_train = mean_absolute_error(ytrain_1d, mean_train_1d)\n",
    "rmse_train = np.sqrt(mean_squared_error(ytrain_1d, mean_train_1d))\n",
    "r2_train = r2_score(ytrain_1d, mean_train_1d)\n",
    "\n",
    "mae_test = mean_absolute_error(ytest_1d, mean_test_1d)\n",
    "rmse_test = np.sqrt(mean_squared_error(ytest_1d, mean_test_1d))\n",
    "r2_test = r2_score(ytest_1d, mean_test_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics = {\n",
    "    'MAE': [mae_train, mae_test],\n",
    "    'RMSE': [rmse_train, rmse_test],\n",
    "    'R2': [r2_train, r2_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c92303-41cd-4da1-b09a-09f21e9cb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure with two subplots: one for train and one for test\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n",
    "\n",
    "# Plotting for the training set on the left subplot\n",
    "axes[0].errorbar(ytrain_1d, mean_train_1d, \n",
    "                 yerr=std_train_1d, \n",
    "                 fmt='o', ecolor='lightgray', mec='blue', mfc='skyblue', \n",
    "                 alpha=0.7, capsize=5, label='Train Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[0].plot(ytrain_1d, ytrain_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[0].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[0].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[0].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=14)\n",
    "axes[0].set_title('Training Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "# Plotting for the test set on the right subplot\n",
    "axes[1].errorbar(ytest_1d, mean_test_1d, \n",
    "                 yerr=std_test_1d, \n",
    "                 fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "                 alpha=0.7, capsize=5, label='Test Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[1].plot(ytest_1d, ytest_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[1].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[1].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[1].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[1].set_title('Test Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[1].legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c554c851",
   "metadata": {},
   "source": [
    "### Evaluation of Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x(c) is an array of confidence levels from 0 to 1 at intervals of 0.01\n",
    "confidence_levels = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Function to calculate the observed confidence\n",
    "def calculate_observed_confidence(y_true, mean_pred, std_pred, z_value):\n",
    "    lower_bound = mean_pred - z_value * std_pred / 2\n",
    "    upper_bound = mean_pred + z_value * std_pred / 2\n",
    "    return np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n",
    "\n",
    "# Calculate the z-scores for the given confidence levels (two-tailed)\n",
    "z_scores = [scipy.stats.norm.ppf((1 + cl) / 2) for cl in confidence_levels]\n",
    "\n",
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytrain, mean_train, std_train, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the observed confidence data to a CSV file\n",
    "GPR_calibration_data = pd.DataFrame({\n",
    "    'Expected_Confidence': confidence_levels,\n",
    "    'GPR_Observed_Confidence_Train': observed_confidence\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytest, mean_test, std_test, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "GPR_calibration_data['GPR_Observed_Confidence_Test'] = observed_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac1c52",
   "metadata": {},
   "source": [
    "### Sparsification plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b17016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices = np.argsort(-std_train)\n",
    "sorted_ytrain = ytrain[sorted_indices]\n",
    "sorted_mean_train = mean_train[sorted_indices]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_train = []\n",
    "fractions = np.arange(0.0, 1.00, 0.001)  # From 2% to 98% in steps of 2%\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytrain))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse = rmse(sorted_ytrain[num_to_remove:], sorted_mean_train[num_to_remove:])\n",
    "    rmse_values_train.append(remaining_rmse)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_train, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Define dataFrame\n",
    "GPR_Sparsification_data = pd.DataFrame({\n",
    "    'Sparsification_fractions': fractions,\n",
    "    'GPR_rmse_values_Train': rmse_values_train\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_test = np.argsort(-std_test)\n",
    "sorted_ytest = ytest[sorted_indices_test]\n",
    "sorted_mean_test = mean_test[sorted_indices_test]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_test = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytest))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_test = rmse(sorted_ytest[num_to_remove:], sorted_mean_test[num_to_remove:])\n",
    "    rmse_values_test.append(remaining_rmse_test)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_test, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "GPR_Sparsification_data['GPR_rmse_values_Test'] = rmse_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090053e7-25e1-49c2-988a-ca05bc2240eb",
   "metadata": {},
   "source": [
    "## OOD_ME data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c414aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_df = pickle.load(open(\"Corr_All.pickle\",\"rb\"))\n",
    "unique_list = pickle.load(open(\"unique_list_All.pickle\",\"rb\"))\n",
    "Columns = pickle.load(open(\"Columns_All.pickle\",\"rb\"))\n",
    "Substructure_list = pickle.load(open(\"polymer.keys_All.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239778fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_ME.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD = data_OOD\n",
    "sns.histplot(df_OOD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to NumPy arrays for xtrain and xtest if they are not already\n",
    "x_OOD_np = xtrain if isinstance(X_OOD, np.ndarray) else X_OOD.values\n",
    "\n",
    "# Reshape ytrain and ytest if they are not already 2D arrays\n",
    "y_OOD_np = y_OOD.reshape(-1, 1) if len(y_OOD.shape) == 1 else y_OOD\n",
    "\n",
    "# Make predictions with normalized features\n",
    "mean_OOD, variance_OOD = model_GPR.predict(x_OOD_np)\n",
    "\n",
    "std_OOD = np.sqrt(variance_OOD)\n",
    "\n",
    "mean_OOD = mean_OOD[:, 0]\n",
    "std_OOD = std_OOD[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de936c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "GPR_calibration_data['GPR_Observed_Confidence_OOD_EXP'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf32542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "#plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "GPR_Sparsification_data['GPR_rmse_values_OOD_EXP'] = rmse_values_OOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Order std_train and divide into two groups\n",
    "std_indices_sorted = np.argsort(std_OOD)\n",
    "lenth = len(y_OOD)\n",
    "half_size = len(y_OOD) // 2\n",
    "small_std_indices = std_indices_sorted[:half_size]\n",
    "large_std_indices = std_indices_sorted[half_size:]\n",
    "\n",
    "# 2. Order actual errors and divide into two groups\n",
    "actual_errors = np.abs(mean_OOD - y_OOD)\n",
    "error_indices_sorted = np.argsort(actual_errors)\n",
    "small_error_indices = error_indices_sorted[:half_size]\n",
    "large_error_indices = error_indices_sorted[half_size:]\n",
    "\n",
    "# 3. Compute the rates for the four different groups\n",
    "# Convert indices to sets for easier comparison\n",
    "small_std_set = set(small_std_indices)\n",
    "large_std_set = set(large_std_indices)\n",
    "small_error_set = set(small_error_indices)\n",
    "large_error_set = set(large_error_indices)\n",
    "\n",
    "# Calculate intersections\n",
    "small_std_small_error = len(small_std_set.intersection(small_error_set)) / lenth\n",
    "small_std_large_error = len(small_std_set.intersection(large_error_set)) / lenth\n",
    "large_std_small_error = len(large_std_set.intersection(small_error_set)) / lenth\n",
    "large_std_large_error = len(large_std_set.intersection(large_error_set)) / lenth\n",
    "\n",
    "# Print the rates\n",
    "print(\"Rate of small std & small actual errors:\", small_std_small_error)\n",
    "print(\"Rate of small std & large actual errors:\", small_std_large_error)\n",
    "print(\"Rate of large std & small actual errors:\", large_std_small_error)\n",
    "print(\"Rate of large std & large actual errors:\", large_std_large_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22e1a4",
   "metadata": {},
   "source": [
    "## OOD_MD data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_MD.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to NumPy arrays for xtrain and xtest if they are not already\n",
    "x_OOD_np = xtrain if isinstance(X_OOD, np.ndarray) else X_OOD.values\n",
    "\n",
    "# Reshape ytrain and ytest if they are not already 2D arrays\n",
    "y_OOD_np = y_OOD.reshape(-1, 1) if len(y_OOD.shape) == 1 else y_OOD\n",
    "\n",
    "# Make predictions with normalized features\n",
    "mean_OOD, variance_OOD = model_GPR.predict(x_OOD_np)\n",
    "\n",
    "std_OOD = np.sqrt(variance_OOD)\n",
    "\n",
    "mean_OOD = mean_OOD[:, 0]\n",
    "std_OOD = std_OOD[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_MD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_MD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_MD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD_MD = mean_absolute_error(y_OOD_MD_1d, mean_OOD_MD_1d)\n",
    "rmse_OOD_MD = np.sqrt(mean_squared_error(y_OOD_MD_1d, mean_OOD_MD_1d))\n",
    "r2_OOD_MD = r2_score(y_OOD_MD_1d, mean_OOD_MD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD_MD = {\n",
    "    'MAE': mae_OOD_MD,\n",
    "    'RMSE': rmse_OOD_MD,\n",
    "    'R2': r2_OOD_MD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_MD_df = pd.DataFrame(metrics_OOD_MD, index=['OOD_MD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_MD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_MD_1d, mean_OOD_MD_1d, \n",
    "            yerr=std_OOD_MD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD_MD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_MD_1d, y_OOD_MD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c392328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "GPR_calibration_data['GPR_Observed_Confidence_OOD_MD'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD_MD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD_MD = y_OOD[sorted_indices_OOD_MD]\n",
    "sorted_mean_OOD_MD = mean_OOD[sorted_indices_OOD_MD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD_MD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD_MD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD_MD = rmse(sorted_y_OOD_MD[num_to_remove:], sorted_mean_OOD_MD[num_to_remove:])\n",
    "    rmse_values_OOD_MD.append(remaining_rmse_OOD_MD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD_MD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "#plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "GPR_Sparsification_data['GPR_rmse_values_OOD_MD'] = rmse_values_OOD_MD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f69e8",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename = '../results/GPR_calibration_data.csv'\n",
    "# Save to CSV\n",
    "GPR_calibration_data.to_csv(filename, index=False)\n",
    "GPR_calibration_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename2 = '../results/GPR_Sparsification_data.csv'\n",
    "# Save to CSV\n",
    "GPR_Sparsification_data.to_csv(filename2, index=False)\n",
    "GPR_Sparsification_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2430760",
   "metadata": {},
   "source": [
    "### high Tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/high_Tg.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00abff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539158ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to NumPy arrays for xtrain and xtest if they are not already\n",
    "x_OOD_np = xtrain if isinstance(X_OOD, np.ndarray) else X_OOD.values\n",
    "\n",
    "# Reshape ytrain and ytest if they are not already 2D arrays\n",
    "y_OOD_np = y_OOD.reshape(-1, 1) if len(y_OOD.shape) == 1 else y_OOD\n",
    "\n",
    "# Make predictions with normalized features\n",
    "mean_OOD, variance_OOD = model_GPR.predict(x_OOD_np)\n",
    "\n",
    "std_OOD = np.sqrt(variance_OOD)\n",
    "\n",
    "mean_OOD = mean_OOD[:, 0]\n",
    "std_OOD = std_OOD[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6352e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ci_multiplier = 1.96  # Multiplier for a 95% confidence interval in a normal distribution\n",
    "lower_bound = mean_OOD - ci_multiplier * std_OOD\n",
    "upper_bound = mean_OOD + ci_multiplier * std_OOD\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'mean_OOD': mean_OOD,\n",
    "    'std_OOD': std_OOD,\n",
    "    '95% CI Lower': lower_bound,\n",
    "    '95% CI Upper': upper_bound\n",
    "})\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "excel_file_path = '../results/high_Tg_GPR.csv'  # Path where the Excel file will be saved\n",
    "df.to_csv(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feea329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "# Create a figure for the OOD set plot\n",
    "# plt.figure(figsize=(7, 6), dpi=1200)\n",
    "fig, ax = plt.subplots(figsize=(5, 4.5), dpi=1200)\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='High Tg Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot((250, 520), (250, 520), 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(250, 520, 40))\n",
    "ax.set_yticks(np.arange(250, 520, 40))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=16, weight='bold')\n",
    "ax.set_ylabel('Predicted Values', fontsize=16, weight='bold')\n",
    "# Fixing the fontsize setting for ticks\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.rc('font', weight='bold')\n",
    "plt.rc('axes', linewidth=2)\n",
    "# Add legend\n",
    "ax.legend(fontsize=font_size, frameon=False)\n",
    "\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figure_high_Tg/GPR.png', format='png', bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ws_UQ_pt)",
   "language": "python",
   "name": "ws_uq_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
