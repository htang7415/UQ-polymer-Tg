{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import spearmanr\n",
    "import scipy\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from math import sqrt\n",
    "\n",
    "from collections import Counter \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7149aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Tg.csv\")\n",
    "\n",
    "molecules = df.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "HashCode = []\n",
    "for i in fp_n:\n",
    "    for j in i.keys():\n",
    "        HashCode.append(j)\n",
    "                \n",
    "unique_set = set(HashCode)\n",
    "unique_list = list(unique_set)\n",
    "Corr_df = pd.DataFrame(unique_list).reset_index()\n",
    "                \n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "        my_finger[index] = polymer[key]\n",
    "    MY_finger.append(my_finger)\n",
    "X = pd.DataFrame(MY_finger)\n",
    "\n",
    "# filter input into the most popular X substructures\n",
    "Zero_Sum = (X == 0).astype(int).sum()\n",
    "NumberOfZero = 6862\n",
    "print(len(Zero_Sum[Zero_Sum < NumberOfZero]))\n",
    "\n",
    "Columns = Zero_Sum[Zero_Sum < NumberOfZero].index\n",
    "Substructure_list = list(polymer.keys())\n",
    "X_count = X[Columns]\n",
    "\n",
    "Y = df['Tg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6016afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"Corr_All.pickle\",\"wb\")\n",
    "pickle.dump(Corr_df, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"unique_list_All.pickle\",\"wb\")\n",
    "pickle.dump(unique_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"polymer.keys_All.pickle\",\"wb\")\n",
    "pickle.dump(Substructure_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Columns_All.pickle\",\"wb\")\n",
    "pickle.dump(Columns, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train, validation, and test sets\n",
    "xtrain, xtemp, ytrain, ytemp = train_test_split(X_count, Y, test_size=0.2, random_state=11)\n",
    "xval, xtest, yval, ytest = train_test_split(xtemp, ytemp, test_size=0.5, random_state=42)\n",
    "bs = 2\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "xtrain_tensor = torch.tensor(xtrain.values).float()\n",
    "ytrain_tensor = torch.tensor(ytrain).float()\n",
    "xval_tensor = torch.tensor(xval.values).float()\n",
    "yval_tensor = torch.tensor(yval).float()\n",
    "xtest_tensor = torch.tensor(xtest.values).float()\n",
    "ytest_tensor = torch.tensor(ytest).float()\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(xtrain_tensor, ytrain_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs , shuffle=True)\n",
    "val_dataset = TensorDataset(xval_tensor, yval_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs )\n",
    "test_dataset = TensorDataset(xtest_tensor, ytest_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9309460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, *neurons):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        prev_neurons = n_input\n",
    "\n",
    "        # Adding hidden layers dynamically based on the neurons tuple\n",
    "        for n in neurons:\n",
    "            layers.append(nn.Linear(prev_neurons, n))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_neurons = n\n",
    "\n",
    "        # Output layer - Assuming it's always one neuron for output\n",
    "        layers.append(nn.Linear(prev_neurons, 1))\n",
    "\n",
    "        # Combine all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with validation step\n",
    "def train_model(model, optimizer, train_loader, val_loader, epochs=150):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Training Loss: {epoch_loss:.4f} - Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Function to evaluate the model and return R^2 score\n",
    "def evaluate_model(models, test_loader):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    total_predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = [model(inputs).cpu().numpy().flatten() for model in models]\n",
    "            total_predictions.append(np.mean(outputs, axis=0))\n",
    "            actuals.extend(labels.numpy().flatten())\n",
    "\n",
    "    total_predictions = np.concatenate(total_predictions)\n",
    "    eva = r2_score(actuals, total_predictions)\n",
    "    return eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENSEMBLES = 20  # Number of models in the ensemble\n",
    "\n",
    "best_hyperparams = {'neurons': (512, 1024, 512)}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the best neuron configuration\n",
    "neurons = best_hyperparams['neurons']\n",
    "print(f\"Training Ensemble with neurons: {neurons}\")\n",
    "\n",
    "models = [NeuralNetwork(xtrain.shape[1], *neurons).to(device) for _ in range(N_ENSEMBLES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    # print(f\"Training Model {i+1}/{N_ENSEMBLES}\")\n",
    "    optimizer = optim.Adam(model.parameters())  # Default learning rate\n",
    "    train_losses, val_losses = train_model(model, optimizer, train_loader, val_loader)\n",
    "\n",
    "# Evaluate ensemble on train data\n",
    "train_r2_score = evaluate_model(models, train_loader)\n",
    "print(f\"R^2 Score on train Data with Ensemble: {train_r2_score}\")\n",
    "\n",
    "# Evaluate ensemble on test data\n",
    "ensemble_r2_score = evaluate_model(models, test_loader)\n",
    "print(f\"R^2 Score on Test Data with Ensemble: {ensemble_r2_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b514ea6-fe37-4220-b632-7c8b75f342e1",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12bac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_ensemble(models, data_loader, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_means, all_stds, all_actuals = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = np.array([model(inputs).cpu().numpy().flatten() for model in models])\n",
    "            \n",
    "            # Calculate mean and std for each data point across all models\n",
    "            mean_predictions = np.mean(outputs, axis=0)\n",
    "            std_predictions = np.std(outputs, axis=0)\n",
    "            all_means.extend(mean_predictions)\n",
    "            all_stds.extend(std_predictions)\n",
    "            all_actuals.extend(labels.numpy().flatten())\n",
    "\n",
    "    return np.array(all_means), np.array(all_stds), np.array(all_actuals)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Predict and compute metrics for training data\n",
    "mean_train, std_train, actual_train = predict_ensemble(models, train_loader, device)\n",
    "r2_train = r2_score(actual_train, mean_train)\n",
    "\n",
    "# Predict and compute metrics for test data\n",
    "mean_test, std_test, actual_test = predict_ensemble(models, test_loader, device)\n",
    "r2_test = r2_score(actual_test, mean_test)\n",
    "\n",
    "print(\"R^2 Score for Training Data:\", r2_train)\n",
    "print(\"R^2 Score for Test Data:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc96c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "ytrain_1d = np.ravel(actual_train)\n",
    "ytest_1d = np.ravel(ytest)\n",
    "mean_train_1d = np.ravel(mean_train)\n",
    "std_train_1d = np.ravel(std_train)\n",
    "mean_test_1d = np.ravel(mean_test)\n",
    "std_test_1d = np.ravel(std_test)\n",
    "\n",
    "# Metric calculation\n",
    "mae_train = mean_absolute_error(ytrain_1d, mean_train_1d)\n",
    "rmse_train = np.sqrt(mean_squared_error(ytrain_1d, mean_train_1d))\n",
    "r2_train = r2_score(ytrain_1d, mean_train_1d)\n",
    "\n",
    "mae_test = mean_absolute_error(ytest_1d, mean_test_1d)\n",
    "rmse_test = np.sqrt(mean_squared_error(ytest_1d, mean_test_1d))\n",
    "r2_test = r2_score(ytest_1d, mean_test_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics = {\n",
    "    'MAE': [mae_train, mae_test],\n",
    "    'RMSE': [rmse_train, rmse_test],\n",
    "    'R2': [r2_train, r2_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e950f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure with two subplots: one for train and one for test\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n",
    "\n",
    "# Plotting for the training set on the left subplot\n",
    "axes[0].errorbar(ytrain_1d, mean_train_1d, \n",
    "                 yerr=std_train_1d, \n",
    "                 fmt='o', ecolor='lightgray', mec='blue', mfc='skyblue', \n",
    "                 alpha=0.7, capsize=5, label='Train Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[0].plot(ytrain_1d, ytrain_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[0].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[0].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[0].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=14)\n",
    "axes[0].set_title('Training Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "# Plotting for the test set on the right subplot\n",
    "axes[1].errorbar(ytest_1d, mean_test_1d, \n",
    "                 yerr=std_test_1d, \n",
    "                 fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "                 alpha=0.7, capsize=5, label='Test Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[1].plot(ytest_1d, ytest_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[1].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[1].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[1].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[1].set_title('Test Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[1].legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = actual_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the training set\n",
    "abs_error_train = abs(ytrain - mean_train)\n",
    "spearman_corr_train, p_value_train = spearmanr(abs_error_train, std_train)\n",
    "\n",
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the test set\n",
    "abs_error_test = abs(ytest - mean_test)\n",
    "spearman_corr_test, p_value_test = spearmanr(abs_error_test, std_test)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results = {\n",
    "    'Spearman_Correlation': [spearman_corr_train, spearman_corr_test],\n",
    "    'p_value': [p_value_train, p_value_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame with 'Train' and 'Test' as index\n",
    "spearman_df = pd.DataFrame(spearman_results, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x(c) is an array of confidence levels from 0 to 1 at intervals of 0.01\n",
    "confidence_levels = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Function to calculate the observed confidence\n",
    "def calculate_observed_confidence(y_true, mean_pred, std_pred, z_value):\n",
    "    lower_bound = mean_pred - z_value * std_pred / 2\n",
    "    upper_bound = mean_pred + z_value * std_pred / 2\n",
    "    return np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n",
    "\n",
    "# Calculate the z-scores for the given confidence levels (two-tailed)\n",
    "z_scores = [scipy.stats.norm.ppf((1 + cl) / 2) for cl in confidence_levels]\n",
    "\n",
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytrain, mean_train, std_train, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the observed confidence data to a CSV file\n",
    "Ensemble_calibration_data = pd.DataFrame({\n",
    "    'Expected_Confidence': confidence_levels,\n",
    "    'Ensemble_Observed_Confidence_Train': observed_confidence\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2824cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytest, mean_test, std_test, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "Ensemble_calibration_data['Ensembel_Observed_Confidence_Test'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices = np.argsort(-std_train)\n",
    "sorted_ytrain = ytrain[sorted_indices]\n",
    "sorted_mean_train = mean_train[sorted_indices]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values = []\n",
    "fractions = np.arange(0, 1.00, 0.001)  # From 2% to 98% in steps of 2%\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytrain))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse = rmse(sorted_ytrain[num_to_remove:], sorted_mean_train[num_to_remove:])\n",
    "    rmse_values.append(remaining_rmse)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Define dataFrame\n",
    "Ensemble_Sparsification_data = pd.DataFrame({\n",
    "    'Sparsification fractions': fractions,\n",
    "    'Ensemble_rmse_values_Train': rmse_values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_test = np.argsort(-std_test)\n",
    "sorted_ytest = ytest[sorted_indices_test]\n",
    "sorted_mean_test = mean_test[sorted_indices_test]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_test = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytest))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_test = rmse(sorted_ytest[num_to_remove:], sorted_mean_test[num_to_remove:])\n",
    "    rmse_values_test.append(remaining_rmse_test)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_test, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "Ensemble_Sparsification_data['Ensemble_rmse_values_Test'] = rmse_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7f927",
   "metadata": {},
   "source": [
    "## OOD data prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6ea74",
   "metadata": {},
   "source": [
    "### EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_df = pickle.load(open(\"Corr_All.pickle\",\"rb\"))\n",
    "unique_list = pickle.load(open(\"unique_list_All.pickle\",\"rb\"))\n",
    "Columns = pickle.load(open(\"Columns_All.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_ME.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD = data_OOD\n",
    "sns.histplot(df_OOD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d441476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and compute metrics for training data\n",
    "mean_OOD, std_OOD, actual_OOD = predict_ensemble(models, OOD_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5eb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "Ensemble_calibration_data['Ensemble_Observed_Confidence_OOD_EXP'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccd4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "Ensemble_Sparsification_data['Ensemble_rmse_values_OOD_EXP'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e8f6c",
   "metadata": {},
   "source": [
    "### MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_MD.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_OOD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672df174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD = data_OOD\n",
    "sns.histplot(df_OOD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and compute metrics for training data\n",
    "mean_OOD, std_OOD, actual_OOD = predict_ensemble(models, OOD_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6506839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83328af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c25190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b79a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c163f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the observed confidence\n",
    "def calculate_observed_confidence(y_true, mean_pred, std_pred, z_value):\n",
    "    lower_bound = mean_pred - z_value * std_pred / 2\n",
    "    upper_bound = mean_pred + z_value * std_pred / 2\n",
    "    return np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n",
    "\n",
    "# Calculate the z-scores for the given confidence levels (two-tailed)\n",
    "z_scores = [scipy.stats.norm.ppf((1 + cl) / 2) for cl in confidence_levels]\n",
    "\n",
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "Ensemble_calibration_data['Ensemble_Observed_Confidence_OOD_MD'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2125b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "Ensemble_Sparsification_data['Ensemble_rmse_values_OOD_MD'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6bff93",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef528db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename = '../results/Ensemble_calibration_data.csv'\n",
    "# Save to CSV\n",
    "Ensemble_calibration_data.to_csv(filename, index=False)\n",
    "Ensemble_calibration_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename2 = '../results/Ensemble_Sparsification_data.csv'\n",
    "# Save to CSV\n",
    "Ensemble_Sparsification_data.to_csv(filename2, index=False)\n",
    "Ensemble_Sparsification_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9547b67",
   "metadata": {},
   "source": [
    "### High Tg results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/high_Tg.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and compute metrics for training data\n",
    "mean_OOD, std_OOD, actual_OOD = predict_ensemble(models, OOD_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca06a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_multiplier = 1.96  # Multiplier for a 95% confidence interval in a normal distribution\n",
    "lower_bound = mean_OOD - ci_multiplier * std_OOD\n",
    "upper_bound = mean_OOD + ci_multiplier * std_OOD\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'mean_OOD': mean_OOD,\n",
    "    'std_OOD': std_OOD,\n",
    "    '95% CI Lower': lower_bound,\n",
    "    '95% CI Upper': upper_bound\n",
    "})\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "excel_file_path = '../results/high_Tg_ensemble.csv'  # Path where the Excel file will be saved\n",
    "df.to_csv(excel_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3601383",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "# Create a figure for the OOD set plot\n",
    "# plt.figure(figsize=(7, 6), dpi=1200)\n",
    "fig, ax = plt.subplots(figsize=(5, 4.5), dpi=1200)\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='High Tg Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot((250, 520), (250, 520), 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(300, 520, 40))\n",
    "ax.set_yticks(np.arange(300, 520, 40))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=16, weight='bold')\n",
    "ax.set_ylabel('Predicted Values', fontsize=16, weight='bold')\n",
    "# Fixing the fontsize setting for ticks\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.rc('font', weight='bold')\n",
    "plt.rc('axes', linewidth=2)\n",
    "# Add legend\n",
    "ax.legend(fontsize=font_size, frameon=False)\n",
    "\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figure_high_Tg/Ensemble.png', format='png', bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ws_UQ_pt)",
   "language": "python",
   "name": "ws_uq_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
