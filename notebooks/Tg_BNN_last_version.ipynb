{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "import time\n",
    "from scipy.stats import spearmanr\n",
    "from numpy.random import seed\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import os\n",
    "# Initialize model and move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Tg.csv\")\n",
    "\n",
    "molecules = df.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "HashCode = []\n",
    "for i in fp_n:\n",
    "    for j in i.keys():\n",
    "        HashCode.append(j)\n",
    "                \n",
    "unique_set = set(HashCode)\n",
    "unique_list = list(unique_set)\n",
    "Corr_df = pd.DataFrame(unique_list).reset_index()\n",
    "                \n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "        my_finger[index] = polymer[key]\n",
    "    MY_finger.append(my_finger)\n",
    "X = pd.DataFrame(MY_finger)\n",
    "\n",
    "# filter input into the most popular X substructures\n",
    "Zero_Sum = (X == 0).astype(int).sum()\n",
    "NumberOfZero = 6862\n",
    "print(len(Zero_Sum[Zero_Sum < NumberOfZero]))\n",
    "\n",
    "Columns = Zero_Sum[Zero_Sum < NumberOfZero].index\n",
    "Substructure_list = list(polymer.keys())\n",
    "X_count = X[Columns]\n",
    "\n",
    "Y = df['Tg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060989d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"Corr_All.pickle\",\"wb\")\n",
    "pickle.dump(Corr_df, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"unique_list_All.pickle\",\"wb\")\n",
    "pickle.dump(unique_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"polymer.keys_All.pickle\",\"wb\")\n",
    "pickle.dump(Substructure_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Columns_All.pickle\",\"wb\")\n",
    "pickle.dump(Columns, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72487625",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtemp, ytrain, ytemp = train_test_split(X_count, Y, test_size=0.2, random_state=11)\n",
    "xval, xtest, yval, ytest = train_test_split(xtemp, ytemp, test_size=0.5, random_state=42)\n",
    "\n",
    "bs = 128\n",
    "\n",
    "# Assuming X and Y are your feature and target datasets respectively\n",
    "xtrain_tensor = torch.tensor(xtrain.values).float()\n",
    "ytrain_tensor = torch.tensor(ytrain).float()\n",
    "xval_tensor = torch.tensor(xval.values).float()\n",
    "yval_tensor = torch.tensor(yval).float()\n",
    "xtest_tensor = torch.tensor(xtest.values).float()\n",
    "ytest_tensor = torch.tensor(ytest).float()\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(xtrain_tensor, ytrain_tensor)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=bs, shuffle=True)\n",
    "val_data = TensorDataset(xval_tensor, yval_tensor)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=bs)\n",
    "test_data = TensorDataset(xtest_tensor, ytest_tensor)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = 512\n",
    "n_2 = 512\n",
    "n_3 = 64\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    # Bayesian Linear layer\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BayesianLinear, self).__init__()\n",
    "        # parameters for the mean\n",
    "        self.mean = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        # parameters for the standard deviation\n",
    "        self.log_std = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the parameters\n",
    "        nn.init.normal_(self.mean, 0, 0.1)\n",
    "        nn.init.normal_(self.log_std, -2.1, 0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample weights\n",
    "        std = torch.exp(self.log_std)\n",
    "        eps = torch.randn_like(std)\n",
    "        weights = self.mean + eps * std\n",
    "        return F.linear(x, weights)\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(BayesianNeuralNetwork, self).__init__()\n",
    "        self.bayesian1 = BayesianLinear(n_features, n_1)\n",
    "        self.bayesian2 = BayesianLinear(n_1, n_2)\n",
    "        self.bayesian3 = BayesianLinear(n_2, n_3)\n",
    "        self.bayesian4 = BayesianLinear(n_3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bayesian1(x))\n",
    "        x = F.relu(self.bayesian2(x))\n",
    "        x = F.relu(self.bayesian3(x))\n",
    "        output = self.bayesian4(x)\n",
    "        mean = output[:, 0]\n",
    "        # Ensure standard deviation is positive\n",
    "        std = torch.exp(output[:, 1].clamp(-10, 5))\n",
    "        # std = torch.exp(output[:, 1])\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "\n",
    "def combined_loss(targets, mean, std, w1, w2):\n",
    "    # Mean Absolute Error term\n",
    "    mae = torch.abs(targets - mean).mean()\n",
    "\n",
    "    # Negative Log-Likelihood term\n",
    "    variance = std**2\n",
    "    nll_first_term = torch.log(2 * torch.pi * variance) / 2\n",
    "    nll_second_term = ((targets - mean)**2) / (2 * variance)\n",
    "    nll = torch.mean(nll_first_term + nll_second_term)\n",
    "\n",
    "    # Combined loss\n",
    "    return w1 * mae + w2 * nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, w1, w2, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            distribution = model(data)\n",
    "            mean, std = distribution.mean, distribution.stddev\n",
    "            loss = combined_loss(target, mean, std, w1, w2)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "model_BNN= BayesianNeuralNetwork(n_features=xtrain.shape[1]).to(device)\n",
    "\n",
    "w1 = 0.0\n",
    "w2 = 1\n",
    "# Train the model\n",
    "train_model(model_BNN, train_loader, w1, w2, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, input_data, n_samples=500):\n",
    "    model.eval()\n",
    "    all_samples = torch.zeros((n_samples, input_data.size(0)))\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            distribution = model(input_data)\n",
    "            all_samples[i] = distribution.sample()  # Sample from the distribution\n",
    "    mean = all_samples.mean(0)\n",
    "    std = all_samples.std(0)\n",
    "    return mean, std\n",
    "\n",
    "# Convert test data to tensor and move to GPU\n",
    "xtest_tensor = torch.tensor(xtest.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Predictions\n",
    "mean_train, std_train = predict_with_uncertainty(model_BNN, xtrain_tensor.to(device))\n",
    "mean_test, std_test = predict_with_uncertainty(model_BNN, xtest_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5414a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = mean_train.numpy()\n",
    "std_train = std_train.numpy()\n",
    "mean_test = mean_test.numpy()\n",
    "std_test = std_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the training set\n",
    "abs_error_train = abs(ytrain - mean_train)\n",
    "spearman_corr_train, p_value_train = spearmanr(abs_error_train, std_train)\n",
    "\n",
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the test set\n",
    "abs_error_test = abs(ytest - mean_test)\n",
    "spearman_corr_test, p_value_test = spearmanr(abs_error_test, std_test)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results = {\n",
    "    'Spearman_Correlation': [spearman_corr_train, spearman_corr_test],\n",
    "    'p_value': [p_value_train, p_value_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame with 'Train' and 'Test' as index\n",
    "spearman_df = pd.DataFrame(spearman_results, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16201b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_train, std_train, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (train)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (train)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (train Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c68d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_test, std_test, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (Test)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (Test)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (test Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f246c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "ytrain_1d = np.ravel(ytrain)\n",
    "ytest_1d = np.ravel(ytest)\n",
    "mean_train_1d = np.ravel(mean_train)\n",
    "std_train_1d = np.ravel(std_train)\n",
    "mean_test_1d = np.ravel(mean_test)\n",
    "std_test_1d = np.ravel(std_test)\n",
    "\n",
    "# Metric calculation\n",
    "mae_train = mean_absolute_error(ytrain_1d, mean_train_1d)\n",
    "rmse_train = np.sqrt(mean_squared_error(ytrain_1d, mean_train_1d))\n",
    "r2_train = r2_score(ytrain_1d, mean_train_1d)\n",
    "\n",
    "mae_test = mean_absolute_error(ytest_1d, mean_test_1d)\n",
    "rmse_test = np.sqrt(mean_squared_error(ytest_1d, mean_test_1d))\n",
    "r2_test = r2_score(ytest_1d, mean_test_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics = {\n",
    "    'MAE': [mae_train, mae_test],\n",
    "    'RMSE': [rmse_train, rmse_test],\n",
    "    'R2': [r2_train, r2_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91729075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure with two subplots: one for train and one for test\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n",
    "\n",
    "# Plotting for the training set on the left subplot\n",
    "axes[0].errorbar(ytrain_1d, mean_train_1d, \n",
    "                 yerr=std_train_1d, \n",
    "                 fmt='o', ecolor='lightgray', mec='blue', mfc='skyblue', \n",
    "                 alpha=0.7, capsize=5, label='Train Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[0].plot(ytrain_1d, ytrain_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[0].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[0].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[0].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=14)\n",
    "axes[0].set_title('Training Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "# Plotting for the test set on the right subplot\n",
    "axes[1].errorbar(ytest_1d, mean_test_1d, \n",
    "                 yerr=std_test_1d, \n",
    "                 fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "                 alpha=0.7, capsize=5, label='Test Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[1].plot(ytest_1d, ytest_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[1].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[1].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[1].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[1].set_title('Test Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[1].legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de866d",
   "metadata": {},
   "source": [
    "### Evaluation of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x(c) is an array of confidence levels from 0 to 1 at intervals of 0.01\n",
    "confidence_levels = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Function to calculate the observed confidence\n",
    "def calculate_observed_confidence(y_true, mean_pred, std_pred, z_value):\n",
    "    lower_bound = mean_pred - z_value * std_pred / 2\n",
    "    upper_bound = mean_pred + z_value * std_pred / 2\n",
    "    return np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n",
    "\n",
    "# Calculate the z-scores for the given confidence levels (two-tailed)\n",
    "z_scores = [scipy.stats.norm.ppf((1 + cl) / 2) for cl in confidence_levels]\n",
    "\n",
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytrain, mean_train, std_train, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the observed confidence data to a CSV file\n",
    "BNN_calibration_data = pd.DataFrame({\n",
    "    'Expected_Confidence': confidence_levels,\n",
    "    'BNN_Observed_Confidence_Train': observed_confidence\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence_test = [calculate_observed_confidence(ytest, mean_test, std_test, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence_test, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "BNN_calibration_data['BNN_Observed_Confidence_Test'] = observed_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67e143",
   "metadata": {},
   "source": [
    "### Sparsification plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495585c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Convert them to NumPy arrays using .numpy() method\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices = np.argsort(-std_train)\n",
    "sorted_ytrain = ytrain[sorted_indices]\n",
    "sorted_mean_train = mean_train[sorted_indices]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values = []\n",
    "fractions = np.arange(0.0, 1, 0.001)  # From 2% to 98% in steps of 2%\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytrain))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse = rmse(sorted_ytrain[num_to_remove:], sorted_mean_train[num_to_remove:])\n",
    "    rmse_values.append(remaining_rmse)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Define dataFrame\n",
    "BNN_Sparsification_data = pd.DataFrame({\n",
    "    'Sparsification_fractions': fractions,\n",
    "    'BNN_rmse_values_Train': rmse_values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7569f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_test = np.argsort(-std_test)\n",
    "sorted_ytest = ytest[sorted_indices_test]\n",
    "sorted_mean_test = mean_test[sorted_indices_test]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_test = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytest))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_test = rmse(sorted_ytest[num_to_remove:], sorted_mean_test[num_to_remove:])\n",
    "    rmse_values_test.append(remaining_rmse_test)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_test, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "BNN_Sparsification_data['BNN_rmse_values_Test'] = rmse_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c17b6",
   "metadata": {},
   "source": [
    "### Out of distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82245",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_df = pickle.load(open(\"Corr_All.pickle\",\"rb\"))\n",
    "unique_list = pickle.load(open(\"unique_list_All.pickle\",\"rb\"))\n",
    "Columns = pickle.load(open(\"Columns_All.pickle\",\"rb\"))\n",
    "Substructure_list = pickle.load(open(\"polymer.keys_All.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f12c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_ME.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bf407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD = data_OOD\n",
    "sns.histplot(df_OOD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OOD, std_OOD = predict_with_uncertainty(model_BNN, x_OOD_tensor.to(device))\n",
    "\n",
    "mean_OOD = mean_OOD.numpy()\n",
    "std_OOD = std_OOD.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "BNN_calibration_data['BNN_Observed_Confidence_OOD_EXP'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "BNN_Sparsification_data['BNN_rmse_values_OOD_EXP'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ccabc8",
   "metadata": {},
   "source": [
    "# OOD_MD data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5eb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_MD.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0940b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OOD, std_OOD = predict_with_uncertainty(model_BNN, x_OOD_tensor.to(device))\n",
    "\n",
    "mean_OOD = mean_OOD.numpy()\n",
    "std_OOD = std_OOD.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "BNN_calibration_data['BNN_Observed_Confidence_OOD_MD'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a520250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "BNN_Sparsification_data['BNN_rmse_values_OOD_MD'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf7cd9",
   "metadata": {},
   "source": [
    "### data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79225f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename = '../results/BNN_calibration_data.csv'\n",
    "# Save to CSV\n",
    "BNN_calibration_data.to_csv(filename, index=False)\n",
    "BNN_calibration_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename2 = '../results/BNN_Sparsification_data.csv'\n",
    "# Save to CSV\n",
    "BNN_Sparsification_data.to_csv(filename2, index=False)\n",
    "BNN_Sparsification_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e012c3",
   "metadata": {},
   "source": [
    "### high Tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/high_Tg.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106588a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OOD, std_OOD = predict_with_uncertainty(model_BNN, x_OOD_tensor.to(device))\n",
    "\n",
    "mean_OOD = mean_OOD.numpy()\n",
    "std_OOD = std_OOD.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba798bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_multiplier = 1.96  # Multiplier for a 95% confidence interval in a normal distribution\n",
    "lower_bound = mean_OOD - ci_multiplier * std_OOD\n",
    "upper_bound = mean_OOD + ci_multiplier * std_OOD\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'mean_OOD': mean_OOD,\n",
    "    'std_OOD': std_OOD,\n",
    "    '95% CI Lower': lower_bound,\n",
    "    '95% CI Upper': upper_bound\n",
    "})\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "excel_file_path = '../results/high_Tg_BNN.csv'  # Path where the Excel file will be saved\n",
    "df.to_csv(excel_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dee4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "# Create a figure for the OOD set plot\n",
    "# plt.figure(figsize=(7, 6), dpi=1200)\n",
    "fig, ax = plt.subplots(figsize=(5, 4.5), dpi=1200)\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='High Tg Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot((250, 520), (250, 520), 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(250, 520, 40))\n",
    "ax.set_yticks(np.arange(250, 520, 40))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=16, weight='bold')\n",
    "ax.set_ylabel('Predicted Values', fontsize=16, weight='bold')\n",
    "# Fixing the fontsize setting for ticks\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.rc('font', weight='bold')\n",
    "plt.rc('axes', linewidth=2)\n",
    "# Add legend\n",
    "ax.legend(fontsize=font_size, frameon=False)\n",
    "\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figure_high_Tg/BNN.png', format='png', bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ws_UQ_pt)",
   "language": "python",
   "name": "ws_uq_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
