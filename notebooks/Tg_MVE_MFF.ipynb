{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "import time\n",
    "from scipy.stats import spearmanr\n",
    "from numpy.random import seed\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "# Initialize model and move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Tg.csv\")\n",
    "\n",
    "molecules = df.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "HashCode = []\n",
    "for i in fp_n:\n",
    "    for j in i.keys():\n",
    "        HashCode.append(j)\n",
    "                \n",
    "unique_set = set(HashCode)\n",
    "unique_list = list(unique_set)\n",
    "Corr_df = pd.DataFrame(unique_list).reset_index()\n",
    "                \n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "        my_finger[index] = polymer[key]\n",
    "    MY_finger.append(my_finger)\n",
    "X = pd.DataFrame(MY_finger)\n",
    "\n",
    "# filter input into the most popular X substructures\n",
    "Zero_Sum = (X == 0).astype(int).sum()\n",
    "NumberOfZero = 6862\n",
    "print(len(Zero_Sum[Zero_Sum < NumberOfZero]))\n",
    "\n",
    "Columns = Zero_Sum[Zero_Sum < NumberOfZero].index\n",
    "Substructure_list = list(polymer.keys())\n",
    "X_count = X[Columns]\n",
    "\n",
    "Y = df['Tg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060989d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"Corr_All.pickle\",\"wb\")\n",
    "pickle.dump(Corr_df, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"unique_list_All.pickle\",\"wb\")\n",
    "pickle.dump(unique_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"polymer.keys_All.pickle\",\"wb\")\n",
    "pickle.dump(Substructure_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Columns_All.pickle\",\"wb\")\n",
    "pickle.dump(Columns, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72487625",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtemp, ytrain, ytemp = train_test_split(X_count, Y, test_size=0.2, random_state=11)\n",
    "xval, xtest, yval, ytest = train_test_split(xtemp, ytemp, test_size=0.5, random_state=42)\n",
    "\n",
    "bs = 4\n",
    "\n",
    "# Assuming X and Y are your feature and target datasets respectively\n",
    "xtrain_tensor = torch.tensor(xtrain.values).float()\n",
    "ytrain_tensor = torch.tensor(ytrain).float()\n",
    "xval_tensor = torch.tensor(xval.values).float()\n",
    "yval_tensor = torch.tensor(yval).float()\n",
    "xtest_tensor = torch.tensor(xtest.values).float()\n",
    "ytest_tensor = torch.tensor(ytest).float()\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(xtrain_tensor, ytrain_tensor)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=bs, shuffle=False)\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=bs)\n",
    "val_data = TensorDataset(xval_tensor, yval_tensor)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=bs)\n",
    "test_data = TensorDataset(xtest_tensor, ytest_tensor)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVE_Model(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MVE_Model, self).__init__()\n",
    "        # Base layers\n",
    "        self.fc1 = nn.Linear(input_shape, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 32)\n",
    "        # Modified output layer for mean and standard deviation\n",
    "        self.output = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.output(x)\n",
    "        # Ensure positive standard deviation\n",
    "        mean = x[:, 0]\n",
    "        std_dev = F.softplus(x[:, 1]) + 1e-6  # Ensure std_dev is positive\n",
    "        return mean, std_dev\n",
    "\n",
    "def combined_loss(targets, mean, std, w1, w2):\n",
    "    # Mean Absolute Error term\n",
    "    mae = torch.abs(targets - mean).mean()\n",
    "\n",
    "    # Negative Log-Likelihood term\n",
    "    variance = std**2\n",
    "    nll_first_term = torch.log(2 * torch.pi * variance) / 2\n",
    "    nll_second_term = ((targets - mean)**2) / (2 * variance)\n",
    "    nll = torch.mean(nll_first_term + nll_second_term)\n",
    "\n",
    "    # Combined loss\n",
    "    return w1 * mae + w2 * nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'xtrain' is your input features and 'device' is your computation device\n",
    "input_shape = xtrain.shape[1]\n",
    "model_MVE = MVE_Model(input_shape).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model_MVE.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    model_MVE.train()  # Set the model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        mean, std = model_MVE(data)  # Forward pass\n",
    "        loss = combined_loss(target, mean, std, w1=0.0, w2=1.0)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        epoch_loss += loss.item() * data.size(0)  # Accumulate the loss\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function remains the same but uses the mean and std_dev from the MVE_Model\n",
    "def calculate_mean_std(loader, model):\n",
    "    means = []\n",
    "    std_devs = []\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for data, _ in loader:\n",
    "            data = data.to(device)\n",
    "            mean, std_dev = model(data)\n",
    "            \n",
    "            means.append(mean.cpu().numpy())  # Store the mean\n",
    "            std_devs.append(std_dev.cpu().numpy())  # Store the standard deviation\n",
    "    \n",
    "    # Convert lists to a single array for the entire dataset\n",
    "    means = np.concatenate(means, axis=0)\n",
    "    std_devs = np.concatenate(std_devs, axis=0)\n",
    "    \n",
    "    return means, std_devs\n",
    "\n",
    "# Calculate for training and test data as before\n",
    "mean_train, std_train = calculate_mean_std(train_loader, model_MVE)\n",
    "mean_test, std_test = calculate_mean_std(test_loader, model_MVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5414a4",
   "metadata": {},
   "source": [
    "mean_train = mean_train.numpy()\n",
    "std_train = std_train.numpy()\n",
    "mean_test = mean_test.numpy()\n",
    "std_test = std_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the training set\n",
    "abs_error_train = abs(ytrain - mean_train)\n",
    "spearman_corr_train, p_value_train = spearmanr(abs_error_train, std_train)\n",
    "\n",
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the test set\n",
    "abs_error_test = abs(ytest - mean_test)\n",
    "spearman_corr_test, p_value_test = spearmanr(abs_error_test, std_test)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results = {\n",
    "    'Spearman_Correlation': [spearman_corr_train, spearman_corr_test],\n",
    "    'p_value': [p_value_train, p_value_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame with 'Train' and 'Test' as index\n",
    "spearman_df = pd.DataFrame(spearman_results, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16201b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_train, std_train, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (train)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (train)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (train Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c68d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_test, std_test, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (Test)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (Test)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (test Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f246c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "ytrain_1d = np.ravel(ytrain)\n",
    "ytest_1d = np.ravel(ytest)\n",
    "mean_train_1d = np.ravel(mean_train)\n",
    "std_train_1d = np.ravel(std_train)\n",
    "mean_test_1d = np.ravel(mean_test)\n",
    "std_test_1d = np.ravel(std_test)\n",
    "\n",
    "# Metric calculation\n",
    "mae_train = mean_absolute_error(ytrain_1d, mean_train_1d)\n",
    "rmse_train = np.sqrt(mean_squared_error(ytrain_1d, mean_train_1d))\n",
    "r2_train = r2_score(ytrain_1d, mean_train_1d)\n",
    "\n",
    "mae_test = mean_absolute_error(ytest_1d, mean_test_1d)\n",
    "rmse_test = np.sqrt(mean_squared_error(ytest_1d, mean_test_1d))\n",
    "r2_test = r2_score(ytest_1d, mean_test_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics = {\n",
    "    'MAE': [mae_train, mae_test],\n",
    "    'RMSE': [rmse_train, rmse_test],\n",
    "    'R2': [r2_train, r2_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91729075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure with two subplots: one for train and one for test\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n",
    "\n",
    "# Plotting for the training set on the left subplot\n",
    "axes[0].errorbar(ytrain_1d, mean_train_1d, \n",
    "                 yerr=std_train_1d, \n",
    "                 fmt='o', ecolor='lightgray', mec='blue', mfc='skyblue', \n",
    "                 alpha=0.7, capsize=5, label='Train Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[0].plot(ytrain_1d, ytrain_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[0].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[0].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[0].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=14)\n",
    "axes[0].set_title('Training Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "# Plotting for the test set on the right subplot\n",
    "axes[1].errorbar(ytest_1d, mean_test_1d, \n",
    "                 yerr=std_test_1d, \n",
    "                 fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "                 alpha=0.7, capsize=5, label='Test Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[1].plot(ytest_1d, ytest_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[1].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[1].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[1].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[1].set_title('Test Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[1].legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de866d",
   "metadata": {},
   "source": [
    "### Evaluation of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x(c) is an array of confidence levels from 0 to 1 at intervals of 0.01\n",
    "confidence_levels = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Function to calculate the observed confidence\n",
    "def calculate_observed_confidence(y_true, mean_pred, std_pred, z_value):\n",
    "    lower_bound = mean_pred - z_value * std_pred / 2\n",
    "    upper_bound = mean_pred + z_value * std_pred / 2\n",
    "    return np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n",
    "\n",
    "# Calculate the z-scores for the given confidence levels (two-tailed)\n",
    "z_scores = [scipy.stats.norm.ppf((1 + cl) / 2) for cl in confidence_levels]\n",
    "\n",
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytrain, mean_train, std_train, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the observed confidence data to a CSV file\n",
    "MVE_calibration_data = pd.DataFrame({\n",
    "    'Expected_Confidence': confidence_levels,\n",
    "    'MVE_Observed_Confidence_Train': observed_confidence\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence_test = [calculate_observed_confidence(ytest, mean_test, std_test, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence_test, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "MVE_calibration_data['MVE_Observed_Confidence_Test'] = observed_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67e143",
   "metadata": {},
   "source": [
    "### Sparsification plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495585c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Convert them to NumPy arrays using .numpy() method\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices = np.argsort(-std_train)\n",
    "sorted_ytrain = ytrain[sorted_indices]\n",
    "sorted_mean_train = mean_train[sorted_indices]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values = []\n",
    "fractions = np.arange(0.0, 1., 0.001)\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytrain))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse = rmse(sorted_ytrain[num_to_remove:], sorted_mean_train[num_to_remove:])\n",
    "    rmse_values.append(remaining_rmse)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Define dataFrame\n",
    "MVE_Sparsification_data = pd.DataFrame({\n",
    "    'Sparsification_fractions': fractions,\n",
    "    'MVE_rmse_values_Train': rmse_values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7569f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_test = np.argsort(-std_test)\n",
    "sorted_ytest = ytest[sorted_indices_test]\n",
    "sorted_mean_test = mean_test[sorted_indices_test]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_test = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytest))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_test = rmse(sorted_ytest[num_to_remove:], sorted_mean_test[num_to_remove:])\n",
    "    rmse_values_test.append(remaining_rmse_test)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_test, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "MVE_Sparsification_data['MVE_rmse_values_Test'] = rmse_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c17b6",
   "metadata": {},
   "source": [
    "### Out of distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82245",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_df = pickle.load(open(\"Corr_All.pickle\",\"rb\"))\n",
    "unique_list = pickle.load(open(\"unique_list_All.pickle\",\"rb\"))\n",
    "Columns = pickle.load(open(\"Columns_All.pickle\",\"rb\"))\n",
    "Substructure_list = pickle.load(open(\"polymer.keys_All.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f12c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_ME.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bf407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD = data_OOD\n",
    "sns.histplot(df_OOD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OOD, std_OOD = calculate_mean_std(OOD_loader, model_MVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "MVE_calibration_data['MVE_Observed_Confidence_OOD_EXP'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "MVE_Sparsification_data['MVE_rmse_values_OOD_EXP'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ccabc8",
   "metadata": {},
   "source": [
    "# OOD_MD data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5eb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/Tg_OOD_MD.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0940b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OOD, std_OOD = calculate_mean_std(OOD_loader, model_MVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "MVE_calibration_data['MVE_Observed_Confidence_OOD_MD'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a520250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "MVE_Sparsification_data['MVE_rmse_values_OOD_MD'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf7cd9",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79225f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename = '../results/MVE_calibration_data.csv'\n",
    "# Save to CSV\n",
    "MVE_calibration_data.to_csv(filename, index=False)\n",
    "MVE_calibration_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename2 = '../results/MVE_Sparsification_data.csv'\n",
    "# Save to CSV\n",
    "MVE_Sparsification_data.to_csv(filename2, index=False)\n",
    "MVE_Sparsification_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea994a4",
   "metadata": {},
   "source": [
    "### experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09776b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv('../data/high_Tg.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_OOD = X_OOD[Columns]\n",
    "\n",
    "y_OOD = data_OOD['Tg'].values\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "x_OOD_tensor = torch.tensor(X_OOD.values).float()\n",
    "y_OOD_tensor = torch.tensor(y_OOD).float()\n",
    "\n",
    "# DataLoaders\n",
    "OOD_dataset = TensorDataset(x_OOD_tensor, y_OOD_tensor)\n",
    "OOD_loader = DataLoader(OOD_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3246730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.3, 'edgecolor':'white'})\n",
    "#plt.figure(figsize=(4,4), dpi= 600)\n",
    "df_OOD_MD = data_OOD\n",
    "sns.histplot(df_OOD_MD['Tg'].dropna(), kde=True, color = 'blue', alpha = 0.3, edgecolor='white')\n",
    "\n",
    "#plt.xlim(-200,500)\n",
    "#plt.legend()\n",
    "plt.xticks(size=18)\n",
    "plt.yticks(size=18)\n",
    "plt.xlabel(\"Tg [℃]\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2357e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OOD, std_OOD = calculate_mean_std(OOD_loader, model_MVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caed2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ci_multiplier = 1.96  # Multiplier for a 95% confidence interval in a normal distribution\n",
    "lower_bound = mean_OOD - ci_multiplier * std_OOD\n",
    "upper_bound = mean_OOD + ci_multiplier * std_OOD\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'mean_OOD': mean_OOD,\n",
    "    'std_OOD': std_OOD,\n",
    "    '95% CI Lower': lower_bound,\n",
    "    '95% CI Upper': upper_bound\n",
    "})\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "excel_file_path = '../results/high_Tg_MVE.csv'  # Path where the Excel file will be saved\n",
    "df.to_csv(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd14f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "# Create a figure for the OOD set plot\n",
    "# plt.figure(figsize=(7, 6), dpi=1200)\n",
    "fig, ax = plt.subplots(figsize=(5, 4.5), dpi=1200)\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='High Tg Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot((250, 520), (250, 520), 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(250, 520, 40))\n",
    "ax.set_yticks(np.arange(250, 520, 40))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=16, weight='bold')\n",
    "ax.set_ylabel('Predicted Values', fontsize=16, weight='bold')\n",
    "# Fixing the fontsize setting for ticks\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.rc('font', weight='bold')\n",
    "plt.rc('axes', linewidth=2)\n",
    "# Add legend\n",
    "ax.legend(fontsize=font_size, frameon=False)\n",
    "\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/figure_high_Tg/MVE.png', format='png', bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ws_UQ_pt)",
   "language": "python",
   "name": "ws_uq_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
