{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727ad7c1-9b03-4a79-a215-fe0a58541b4a",
   "metadata": {},
   "source": [
    "# Evidential deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbacecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import scipy\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f1875-6a5a-4966-a66c-f5857eea06b5",
   "metadata": {},
   "source": [
    "## Step 1: Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17a2d0-0940-41c0-8b39-ce9b04a684ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\Tg.csv')\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=11)\n",
    "test_data, val_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the datasets\n",
    "train_data.to_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\MFF\\Tg_train.csv', index=False)\n",
    "test_data.to_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\MFF\\Tg_test.csv', index=False)\n",
    "val_data.to_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\MFF\\Tg_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef646ce",
   "metadata": {},
   "source": [
    "### Data processing for MFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a29e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_df = pickle.load(open(\"../data/Tg_UQ/MFF/Corr_All.pickle\",\"rb\"))\n",
    "unique_list = pickle.load(open(\"../data/Tg_UQ/MFF/unique_list_All.pickle\",\"rb\"))\n",
    "Columns = pickle.load(open(\"../data/Tg_UQ/MFF/Columns_All.pickle\",\"rb\"))\n",
    "Substructure_list = pickle.load(open(\"../data/Tg_UQ/MFF/polymer.keys_All.pickle\",\"rb\"))\n",
    "\n",
    "data_train = pd.read_csv('../data/Tg_UQ/MFF/Tg_train.csv')\n",
    "\n",
    "molecules = data_train.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_train = pd.DataFrame(MY_finger)\n",
    "X_train = X_train[Columns]\n",
    "\n",
    "X_train.to_csv(\"../data/Tg_UQ/MFF/train_MFF.csv\", index=False)\n",
    "\n",
    "data_test = pd.read_csv('../data/Tg_UQ/MFF/Tg_test.csv')\n",
    "\n",
    "molecules = data_test.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_test = pd.DataFrame(MY_finger)\n",
    "X_test = X_test[Columns]\n",
    "\n",
    "X_test.to_csv(\"../data/Tg_UQ/MFF/test_MFF.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a979c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b2d81",
   "metadata": {},
   "source": [
    "# 1.5 Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a9e46",
   "metadata": {},
   "source": [
    "## optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7afb1",
   "metadata": {},
   "source": [
    "python hyperparameter_optimization.py --data_path data/Tg_UQ/MF/Tg_train.csv --dataset_type regression --num_iters 100 --config_save_path configs/Tg_MF_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eeb680",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7703f",
   "metadata": {},
   "source": [
    "python train.py --data_path <data_path> --dataset_type <type> --config_path <config_path>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d17d32-eef0-4df6-a94b-10ecb3b8234a",
   "metadata": {},
   "source": [
    "## Step 2:Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fc1af",
   "metadata": {},
   "source": [
    "### MFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6ae0a",
   "metadata": {},
   "source": [
    "python train.py --confidence evidence --epochs 250 --new_loss --regularizer_coeff 0.2 --save_dir results/evidence_demo --save_confidence conf.txt --confidence_evaluation_methods cutoff --split_type random --split_sizes 0.8 0.1 0.1 --seed 0 --dataset_type regression --data_path data/Tg_UQ/MFF/Tg_train.csv --features_path data/Tg_UQ/MFF/train_MFF.csv --no_features_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262dce1-9328-4cfa-85ae-54631b141904",
   "metadata": {},
   "source": [
    "## Step 3: Run Prediction\n",
    "### command line and need to change the name of folder every time: for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c295e-9401-4acc-b4a8-5d55ffc0e4ad",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 training set\n",
    "\n",
    "### MFF\n",
    "\n",
    "python predict.py --test_path data/Tg_UQ/MFF/Tg_train.csv --features_path data/Tg_UQ/MFF/train_MFF.csv --preds_path results/Tg_UQ/MFF/Tg_train_result.csv --checkpoint_path results\\evidence_demo\\240201-210537535451_Tg_train_evidence_MFF\\fold_0\\model_0\\model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb639e",
   "metadata": {},
   "source": [
    "## 3.2 test set\n",
    "\n",
    "### MFF\n",
    "python predict.py --test_path data/Tg_UQ/MFF/Tg_test.csv --features_path data/Tg_UQ/MFF/test_MFF.csv --preds_path results/Tg_UQ/MFF/Tg_test_result.csv --checkpoint_path results\\evidence_demo\\240201-210537535451_Tg_train_evidence_MFF\\fold_0\\model_0\\model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97035a",
   "metadata": {},
   "source": [
    "### 3.3 OOD_ME data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529c986",
   "metadata": {},
   "source": [
    "### MFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac69dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_OOD = pd.read_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\MF\\Tg_OOD_ME.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_EXP = X_OOD[Columns]\n",
    "\n",
    "X_EXP.to_csv(\"data/Tg_UQ/MFF/OOD_EXP_MFF.csv\", index=False)\n",
    "\n",
    "data_OOD = pd.read_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\MF\\Tg_OOD_MD.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_MD = X_OOD[Columns]\n",
    "\n",
    "X_MD.to_csv(\"../data/Tg_UQ/MFF/OOD_MD_MFF.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OOD = pd.read_csv(r'chemprop-confidence-evidential\\data\\Tg_UQ\\MFF\\high_Tg.csv')\n",
    "\n",
    "molecules = data_OOD.Smiles.apply(Chem.MolFromSmiles)\n",
    "fp = molecules.apply(lambda m: AllChem.GetMorganFingerprint(m, radius=3))\n",
    "fp_n = fp.apply(lambda m: m.GetNonzeroElements())\n",
    "MY_finger = []\n",
    "for polymer in fp_n:\n",
    "    my_finger = [0] * len(unique_list)\n",
    "    for key in polymer.keys():\n",
    "        if key in list(Corr_df[0]):\n",
    "            index = Corr_df[Corr_df[0] == key]['index'].values[0]\n",
    "            my_finger[index] = polymer[key]         \n",
    "    MY_finger.append(my_finger)\n",
    "X_OOD = pd.DataFrame(MY_finger)\n",
    "X_MD = X_OOD[Columns]\n",
    "\n",
    "X_MD.to_csv(\"../data/Tg_UQ/MFF/high_Tg_MFF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f419c",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccddf9b",
   "metadata": {},
   "source": [
    "### EXP\n",
    "\n",
    "python predict.py --test_path data/Tg_UQ/MFF/Tg_OOD_ME.csv --features_path data/Tg_UQ/MFF/OOD_EXP_MFF.csv --preds_path results/Tg_UQ/MFF/Tg_OOD_EXP_result.csv --checkpoint_path results\\evidence_demo\\240201-210537535451_Tg_train_evidence_MFF\\fold_0\\model_0\\model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725d33b",
   "metadata": {},
   "source": [
    "### MD\n",
    "\n",
    "python predict.py --test_path data/Tg_UQ/MFF/Tg_OOD_MD.csv --features_path data/Tg_UQ/MFF/OOD_MD_MFF.csv --preds_path results/Tg_UQ/MFF/Tg_OOD_MD_result.csv --checkpoint_path results\\evidence_demo\\240201-210537535451_Tg_train_evidence_MFF\\fold_0\\model_0\\model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20799097",
   "metadata": {},
   "source": [
    "### High Tg\n",
    "\n",
    "python predict.py --test_path data/Tg_UQ/MFF/high_Tg.csv --features_path data/Tg_UQ/MFF/high_Tg_MFF.csv --preds_path results/Tg_UQ/MFF/Tg_high_Tg_result.csv --checkpoint_path results\\evidence_demo\\240201-210537535451_Tg_train_evidence_MFF\\fold_0\\model_0\\model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5febd9-dbcb-4b89-a0b4-6badd5937a94",
   "metadata": {},
   "source": [
    "## Step 4: Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0d58c",
   "metadata": {},
   "source": [
    "### 4.1 results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ca7d9-1c98-4a52-9bfd-ca93492c39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train data\n",
    "# Load predictions and actual values\n",
    "train_result = pd.read_csv(r'chemprop-confidence-evidential\\results\\Tg_UQ\\MFF\\Tg_train_result.csv')\n",
    "# Drop rows with any empty cells\n",
    "train_result.dropna(inplace=True)\n",
    "# Rename the columns as per the new names\n",
    "train_result.columns = ['smiles', 'Tg', 'true_Tg', 'uncertainty', 'std']\n",
    "ytrain = train_result['true_Tg']\n",
    "mean_train = train_result['Tg']\n",
    "std_train = train_result['std']\n",
    "\n",
    "# test data\n",
    "# Load predictions and actual values\n",
    "test_result = pd.read_csv(r'chemprop-confidence-evidential\\results\\Tg_UQ\\MFF\\Tg_test_result.csv')\n",
    "# Drop rows with any empty cells\n",
    "test_result.dropna(inplace=True)\n",
    "# Rename the columns as per the new names\n",
    "test_result.columns = ['smiles', 'Tg', 'true_Tg', 'uncertainty', 'std']\n",
    "ytest = test_result['true_Tg']\n",
    "mean_test = test_result['Tg']\n",
    "std_test = test_result['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2512c0f-2254-44a9-82b8-7af5b0539468",
   "metadata": {},
   "source": [
    "### 4.2 postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230a748-fb57-4141-bd9a-524df5b9359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the training set\n",
    "abs_error_train = abs(ytrain - mean_train)\n",
    "spearman_corr_train, p_value_train = spearmanr(abs_error_train, std_train)\n",
    "\n",
    "# Calculate absolute errors and Spearman's Rank Correlation Coefficient for the test set\n",
    "abs_error_test = abs(ytest - mean_test)\n",
    "spearman_corr_test, p_value_test = spearmanr(abs_error_test, std_test)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results = {\n",
    "    'Spearman_Correlation': [spearman_corr_train, spearman_corr_test],\n",
    "    'p_value': [p_value_train, p_value_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame with 'Train' and 'Test' as index\n",
    "spearman_df = pd.DataFrame(spearman_results, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_train, std_train, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (train)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (train)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (train Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot+\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ed290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(abs_error_test, std_test, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (Test)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (Test)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (test Set)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e32ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "ytrain_1d = np.ravel(ytrain)\n",
    "ytest_1d = np.ravel(ytest)\n",
    "mean_train_1d = np.ravel(mean_train)\n",
    "std_train_1d = np.ravel(std_train)\n",
    "mean_test_1d = np.ravel(mean_test)\n",
    "std_test_1d = np.ravel(std_test)\n",
    "\n",
    "# Metric calculation\n",
    "mae_train = mean_absolute_error(ytrain_1d, mean_train_1d)\n",
    "rmse_train = np.sqrt(mean_squared_error(ytrain_1d, mean_train_1d))\n",
    "r2_train = r2_score(ytrain_1d, mean_train_1d)\n",
    "\n",
    "mae_test = mean_absolute_error(ytest_1d, mean_test_1d)\n",
    "rmse_test = np.sqrt(mean_squared_error(ytest_1d, mean_test_1d))\n",
    "r2_test = r2_score(ytest_1d, mean_test_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics = {\n",
    "    'MAE': [mae_train, mae_test],\n",
    "    'RMSE': [rmse_train, rmse_test],\n",
    "    'R2': [r2_train, r2_test]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e219bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure with two subplots: one for train and one for test\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n",
    "\n",
    "# Plotting for the training set on the left subplot\n",
    "axes[0].errorbar(ytrain_1d, mean_train_1d, \n",
    "                 yerr=std_train_1d, \n",
    "                 fmt='o', ecolor='lightgray', mec='blue', mfc='skyblue', \n",
    "                 alpha=0.7, capsize=5, label='Train Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[0].plot(ytrain_1d, ytrain_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[0].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[0].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[0].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=14)\n",
    "axes[0].set_title('Training Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "# Plotting for the test set on the right subplot\n",
    "axes[1].errorbar(ytest_1d, mean_test_1d, \n",
    "                 yerr=std_test_1d, \n",
    "                 fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "                 alpha=0.7, capsize=5, label='Test Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "axes[1].plot(ytest_1d, ytest_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "axes[1].set_xticks(np.arange(-100, 401, 100))\n",
    "axes[1].set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "axes[1].set_xlabel('Actual Values', fontsize=14)\n",
    "axes[1].set_title('Test Set Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "axes[1].legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726bcd7",
   "metadata": {},
   "source": [
    "### Evaluation of Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming x(c) is an array of confidence levels from 0 to 1 at intervals of 0.01\n",
    "confidence_levels = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Function to calculate the observed confidence\n",
    "def calculate_observed_confidence(y_true, mean_pred, std_pred, z_value):\n",
    "    lower_bound = mean_pred - z_value * std_pred / 2\n",
    "    upper_bound = mean_pred + z_value * std_pred / 2\n",
    "    return np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n",
    "\n",
    "# Calculate the z-scores for the given confidence levels (two-tailed)\n",
    "z_scores = [scipy.stats.norm.ppf((1 + cl) / 2) for cl in confidence_levels]\n",
    "\n",
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytrain, mean_train, 1*std_train, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the observed confidence data to a CSV file\n",
    "EDL_calibration_data = pd.DataFrame({\n",
    "    'Expected_Confidence': confidence_levels,\n",
    "    'EDL_Observed_Confidence_Train': observed_confidence\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a80fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(ytest, mean_test, 1*std_test, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "EDL_calibration_data['EDL_Observed_Confidence_Test'] = observed_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb31b0",
   "metadata": {},
   "source": [
    "### Sparsification plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20198c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices = np.argsort(-std_train)\n",
    "sorted_ytrain = ytrain[sorted_indices]\n",
    "sorted_mean_train = mean_train[sorted_indices]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_train = []\n",
    "fractions = np.arange(0, 1.00, 0.001)  # From 2% to 98% in steps of 2%\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytrain))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse = rmse(sorted_ytrain[num_to_remove:], sorted_mean_train[num_to_remove:])\n",
    "    rmse_values_train.append(remaining_rmse)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_train, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Define dataFrame\n",
    "EDL_Sparsification_data = pd.DataFrame({\n",
    "    'Sparsification_fractions': fractions,\n",
    "    'EDL_rmse_values_Train': rmse_values_train\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcf012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_test = np.argsort(-std_test)\n",
    "sorted_ytest = ytest[sorted_indices_test]\n",
    "sorted_mean_test = mean_test[sorted_indices_test]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_test = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_ytest))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_test = rmse(sorted_ytest[num_to_remove:], sorted_mean_test[num_to_remove:])\n",
    "    rmse_values_test.append(remaining_rmse_test)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_test, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "EDL_Sparsification_data['EDL_rmse_values_Test'] = rmse_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478bcc7",
   "metadata": {},
   "source": [
    "### OOD_ME data postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD data\n",
    "# Load predictions and actual values\n",
    "OOD_result = pd.read_csv(r'chemprop-confidence-evidential\\results\\Tg_UQ\\MFF\\Tg_OOD_EXP_result.csv')\n",
    "# Drop rows with any empty cells\n",
    "OOD_result.dropna(inplace=True)\n",
    "# Rename the columns as per the new names\n",
    "OOD_result.columns = ['smiles', 'Tg', 'true_Tg', 'uncertainty', 'std']\n",
    "y_OOD = OOD_result['true_Tg']\n",
    "mean_OOD = OOD_result['Tg']\n",
    "std_OOD = OOD_result['std']\n",
    "\n",
    "print(mean_OOD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD - mean_OOD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(abs_error_OOD, std_OOD, alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Absolute Error (OOD)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (OOD)', fontsize=14)\n",
    "plt.title('Absolute Error vs Standard Deviation (OOD)', fontsize=16)\n",
    "\n",
    "# Optionally, add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a956c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_1d, y_OOD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD, mean_OOD, 1*std_OOD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "EDL_calibration_data['EDL_Observed_Confidence_OOD_EXP'] = observed_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d43cc",
   "metadata": {},
   "source": [
    "### Sparsification plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD = np.argsort(-std_OOD)\n",
    "sorted_y_OOD = y_OOD[sorted_indices_OOD]\n",
    "sorted_mean_OOD = mean_OOD[sorted_indices_OOD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD = rmse(sorted_y_OOD[num_to_remove:], sorted_mean_OOD[num_to_remove:])\n",
    "    rmse_values_OOD.append(remaining_rmse_OOD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "EDL_Sparsification_data['EDL_rmse_values_OOD_EXP'] = rmse_values_OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d1bb9",
   "metadata": {},
   "source": [
    "## OOD_MD data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD data\n",
    "# Load predictions and actual values\n",
    "OOD_MD_result = pd.read_csv(r'D:\\htang\\OneDrive - UW-Madison\\Research\\UQ\\Tg\\UQ_model\\chemprop-confidence-evidential\\results\\Tg_UQ\\MFF\\Tg_OOD_MD_result.csv')\n",
    "# Drop rows with any empty cells\n",
    "OOD_MD_result.dropna(inplace=True)\n",
    "# Rename the columns as per the new names\n",
    "OOD_MD_result.columns = ['smiles', 'Tg', 'true_Tg', 'uncertainty', 'std']\n",
    "y_OOD_MD = OOD_MD_result['true_Tg']\n",
    "mean_OOD_MD = OOD_MD_result['Tg']\n",
    "std_OOD_MD = OOD_MD_result['std']\n",
    "\n",
    "print(mean_OOD_MD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbaa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_MD_1d = np.ravel(y_OOD_MD)\n",
    "mean_OOD_MD_1d = np.ravel(mean_OOD_MD)\n",
    "std_OOD_MD_1d = np.ravel(std_OOD_MD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD_MD = mean_absolute_error(y_OOD_MD_1d, mean_OOD_MD_1d)\n",
    "rmse_OOD_MD = np.sqrt(mean_squared_error(y_OOD_MD_1d, mean_OOD_MD_1d))\n",
    "r2_OOD_MD = r2_score(y_OOD_MD_1d, mean_OOD_MD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD_MD = {\n",
    "    'MAE': mae_OOD_MD,\n",
    "    'RMSE': rmse_OOD_MD,\n",
    "    'R2': r2_OOD_MD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_MD_df = pd.DataFrame(metrics_OOD_MD, index=['OOD_MD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_MD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the OOD set plot\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_MD_1d, mean_OOD_MD_1d, \n",
    "            yerr=std_OOD_MD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='OOD_MD data Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot(y_OOD_MD_1d, y_OOD_MD_1d, 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(-100, 401, 100))\n",
    "ax.set_yticks(np.arange(-100, 401, 100))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=14)\n",
    "ax.set_ylabel('Predicted Values', fontsize=14)\n",
    "ax.set_title('OOD data Predictions - Tg', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe908e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman's Rank Correlation Coefficient for OOD data\n",
    "abs_error_OOD = abs(y_OOD_MD - mean_OOD_MD)\n",
    "spearman_corr_OOD, p_value_OOD = spearmanr(abs_error_OOD, std_OOD_MD)\n",
    "\n",
    "# Organize the results in a dictionary\n",
    "spearman_results_OOD = {\n",
    "    'Spearman_Correlation': [spearman_corr_OOD],\n",
    "    'P_value': [p_value_OOD]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "spearman_df_OOD = pd.DataFrame(spearman_results_OOD, index=['OOD'])\n",
    "\n",
    "# Display the DataFrame\n",
    "spearman_df_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed confidence for each z-score\n",
    "observed_confidence = [calculate_observed_confidence(y_OOD_MD, mean_OOD_MD, 1*std_OOD_MD, z) for z in z_scores]\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(confidence_levels, observed_confidence, label='Calibration curve')\n",
    "plt.plot(confidence_levels, confidence_levels, 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Expected confidence')\n",
    "plt.ylabel('Observed confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "EDL_calibration_data['EDL Observed_Confidence OOD_MD'] = observed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41637943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE using mean_squared_error from sklearn\n",
    "# Step 1: Sort samples by descending order of predictive uncertainty (standard deviation)\n",
    "sorted_indices_OOD_MD = np.argsort(-std_OOD_MD)\n",
    "sorted_y_OOD_MD = y_OOD_MD[sorted_indices_OOD_MD]\n",
    "sorted_mean_OOD_MD = mean_OOD_MD[sorted_indices_OOD_MD]\n",
    "\n",
    "# Step 2 and 3: Remove subsets of samples and calculate RMSE\n",
    "rmse_values_OOD_MD = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    # Calculate the number of samples to remove\n",
    "    num_to_remove = int(fraction * len(sorted_y_OOD_MD))\n",
    "    # Calculate RMSE on the remaining samples\n",
    "    remaining_rmse_OOD_MD = rmse(sorted_y_OOD_MD[num_to_remove:], sorted_mean_OOD_MD[num_to_remove:])\n",
    "    rmse_values_OOD_MD.append(remaining_rmse_OOD_MD)\n",
    "\n",
    "# Step 4: Plot the error metric vs. fraction of removed samples\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fractions, rmse_values_OOD_MD, marker='o')\n",
    "plt.xlabel('Fraction of samples removed')\n",
    "plt.ylabel('RMSE on remaining samples')\n",
    "plt.title('Sparsification Plot')\n",
    "#plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the data\n",
    "EDL_Sparsification_data['EDL_rmse_values_OOD_MD'] = rmse_values_OOD_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to CSV\n",
    "EDL_calibration_data.to_csv(\"results/Tg_UQ/MFF/EDL_calibration_data.csv\", index=False)\n",
    "\n",
    "EDL_calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3076cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDL_Sparsification_data.to_csv(\"results/Tg_UQ/MFF/EDL_Sparsification_data.csv\", index=False)\n",
    "EDL_Sparsification_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e6ef6",
   "metadata": {},
   "source": [
    "### High Tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD data\n",
    "# Load predictions and actual values\n",
    "OOD_result = pd.read_csv(r'chemprop-confidence-evidential\\results\\Tg_UQ\\MFF\\Tg_high_Tg_result.csv')\n",
    "# Drop rows with any empty cells\n",
    "OOD_result.dropna(inplace=True)\n",
    "# Rename the columns as per the new names\n",
    "OOD_result.columns = ['smiles', 'Tg', 'true_Tg', 'uncertainty', 'std']\n",
    "y_OOD = OOD_result['true_Tg']\n",
    "mean_OOD = OOD_result['Tg']\n",
    "std_OOD = OOD_result['std']\n",
    "\n",
    "print(mean_OOD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212759f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that ytest, mean_predictions, and std_dev_predictions are 1D arrays\n",
    "y_OOD_1d = np.ravel(y_OOD)\n",
    "mean_OOD_1d = np.ravel(mean_OOD)\n",
    "std_OOD_1d = np.ravel(std_OOD)\n",
    "\n",
    "# Metric calculation\n",
    "mae_OOD = mean_absolute_error(y_OOD_1d, mean_OOD_1d)\n",
    "rmse_OOD = np.sqrt(mean_squared_error(y_OOD_1d, mean_OOD_1d))\n",
    "r2_OOD = r2_score(y_OOD_1d, mean_OOD_1d)\n",
    "\n",
    "# Organize the metrics into a dictionary with three keys for the three metrics\n",
    "metrics_OOD = {\n",
    "    'MAE': mae_OOD,\n",
    "    'RMSE': rmse_OOD,\n",
    "    'R2': r2_OOD\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_OOD_df = pd.DataFrame(metrics_OOD, index=['OOD test'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_OOD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ci_multiplier = 1.96  # Multiplier for a 95% confidence interval in a normal distribution\n",
    "lower_bound = mean_OOD - ci_multiplier * std_OOD\n",
    "upper_bound = mean_OOD + ci_multiplier * std_OOD\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'mean_OOD': mean_OOD,\n",
    "    'std_OOD': std_OOD,\n",
    "    '95% CI Lower': lower_bound,\n",
    "    '95% CI Upper': upper_bound\n",
    "})\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "excel_file_path = 'results/Tg_UQ/MFF/high_Tg_EDL.csv'  # Path where the Excel file will be saved\n",
    "df.to_csv(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "# Create a figure for the OOD set plot\n",
    "# plt.figure(figsize=(7, 6), dpi=1200)\n",
    "fig, ax = plt.subplots(figsize=(5, 4.5), dpi=1200)\n",
    "# Plotting for the test set\n",
    "ax.errorbar(y_OOD_1d, mean_OOD_1d, \n",
    "            yerr=std_OOD_1d, \n",
    "            fmt='o', ecolor='lightblue', mec='green', mfc='lightgreen', \n",
    "            alpha=0.7, capsize=5, label='High Tg Prediction')\n",
    "\n",
    "# Plot a line for perfect predictions for reference\n",
    "ax.plot((250, 520), (250, 520), 'r--', label='Perfect predictions')\n",
    "\n",
    "# Define the ticks for the x and y axes\n",
    "ax.set_xticks(np.arange(250, 520, 40))\n",
    "ax.set_yticks(np.arange(250, 520, 40))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Actual Values', fontsize=16, weight='bold')\n",
    "ax.set_ylabel('Predicted Values', fontsize=16, weight='bold')\n",
    "# Fixing the fontsize setting for ticks\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.rc('font', weight='bold')\n",
    "plt.rc('axes', linewidth=2)\n",
    "# Add legend\n",
    "ax.legend(fontsize=font_size, frameon=False)\n",
    "\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/Tg_UQ/MFF/EDL.png', format='png', bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UQ",
   "language": "python",
   "name": "uq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
